defaults:
  - base_config

# Hydra configuration
hydra:
  job:
    name: eval

# Override data module to use BehaviorLeRobotDataset for evaluation
data:
  _target_: lara.data.BehaviorDataModule
  data_path: ${oc.env:DATA_DIR}  # Read from .env file
  batch_size: 1  # Process one frame at a time for eval GT generation
  val_batch_size: 1
  split_dataset: false  # No train/val split - use all data for validation only
  val_split_ratio: 0.9  # Ignored when split_dataset is false
  dataloader_num_workers: 1
  max_num_demos: null
  dataset_class: omnigibson.learning.datas.BehaviorLeRobotDataset
  # Dataset-specific parameters (passed via **kwargs to dataset)
  repo_id: "behavior-1k/2025-challenge-demos"
  tasks: ["picking_up_trash"]  # List of task names to load, or null for all tasks
  episodes: 5  # null = all episodes, or use range(N) for first N episodes, or [0, 2, 5] for specific episodes
  modalities: ["rgb", "depth", "seg_instance_id"]  # Modalities to load
  cameras: ["head", "left_wrist", "right_wrist"]  # Cameras to load
  local_only: false  # Set to true to avoid downloading from HuggingFace
  force_cache_sync: false  # Force sync cache with HuggingFace (set false after first download)
  chunk_streaming_using_keyframe: true  # Efficient data access
  # Temporal queries for proprioceptive history
  delta_timestamps:
    observation.state: [-0.0667, -0.0333, 0.0]  # Last 3 states at 30Hz
    action: [-0.0667, -0.0333, 0.0]  # Last 3 actions at 30Hz

# Planner - inherits from base_config but add prompts
planner:
  prompts: ${subtask_prediction}

# Evaluator Configuration
evaluator:
  _target_: lara.llm_evaluator.evaluator.Evaluator
  model: "claude-3-haiku-20240307"  # Cheapest Claude model
  temperature: 0.7
  max_tokens: 500
  api_key_env: "ANTHROPIC_API_KEY"
  prompts: ${llm_evaluator}  # Pass prompts config to evaluator

# Experiment Configuration
experiment_name: "default_experiment"  # Name for this experiment run

# Writer Configuration
writer:
  _target_: lara.llm_evaluator.writer.Writer
  output_dir: "${hydra:runtime.cwd}/eval_outputs/${experiment_name}"
  output_filename: "eval_gt.jsonl"
  metadata_filename: "metadata.json"

# Evaluation specific parameters
eval:
  max_iterations: 10  # Maximum number of batches to process (set to large number or null for all)
  use_stuck_detection: true  # Include stuck detection in prompts
  velocity_threshold: 0.01  # Threshold for detecting if robot is stuck
  save_images: false  # Whether to save images alongside JSONL
  image_dir: "${hydra:runtime.cwd}/eval_outputs/${experiment_name}/images"
